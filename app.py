# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FRkzO0QiOnkoVEfk_LWXw_50ug-kR0q8
"""

pip install flask flask_cors torch transformers sentence-transformers whisper opencv-python

!pip install flask_cors

!pip install sentence-transformers # First, install the sentence-transformers package.
!pip install -U openai-whisper
import whisper


from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
from transformers import VideoMAEFeatureExtractor, VideoMAEModel, AutoTokenizer, GPT2LMHeadModel
from sentence_transformers import SentenceTransformer
import whisper
import cv2
import torch.nn as nn

app = Flask(__name__)
CORS(app)  # Enable CORS for cross-origin requests

# Load models
video_model = VideoMAEModel.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics")
audio_model = whisper.load_model("base") # Now use the updated 'load_model' interface
gpt_tokenizer = AutoTokenizer.from_pretrained("gpt2")
gpt_model = GPT2LMHeadModel.from_pretrained("gpt2")
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")

# ... (rest of your code)
from flask import Flask, request, jsonify
from flask_cors import CORS
import torch
from transformers import VideoMAEFeatureExtractor, VideoMAEModel, AutoTokenizer, GPT2LMHeadModel
from sentence_transformers import SentenceTransformer # Now, you should be able to import SentenceTransformer.
import whisper
import cv2
import torch.nn as nn

app = Flask(__name__)
CORS(app)  # Enable CORS for cross-origin requests

# Load models
video_model = VideoMAEModel.from_pretrained("MCG-NJU/videomae-base-finetuned-kinetics")
audio_model = whisper.load_model("base")
gpt_tokenizer = AutoTokenizer.from_pretrained("gpt2")
gpt_model = GPT2LMHeadModel.from_pretrained("gpt2")
sentence_model = SentenceTransformer("all-MiniLM-L6-v2")

# Projections for embeddings
video_projection = nn.Linear(768, 512)
audio_projection = nn.Linear(384, 512)

# Video feature extraction function
def extract_video_features(video_path, num_frames=16):
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    frame_interval = max(1, total_frames // num_frames)

    frames = []
    for i in range(num_frames):
        cap.set(cv2.CAP_PROP_POS_FRAMES, i * frame_interval)
        success, frame = cap.read()
        if not success:
            break
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        frames.append(frame_rgb)
    cap.release()

    while len(frames) < num_frames:
        frames.append(frames[-1])

    feature_extractor = VideoMAEFeatureExtractor.from_pretrained("MCG-NJU/videomae-base")
    processed_frames = feature_extractor(frames, return_tensors="pt")

    with torch.no_grad():
        video_embeddings = video_model(pixel_values=processed_frames['pixel_values']).last_hidden_state
        video_embedding_avg = video_embeddings.mean(dim=1)

    return video_embedding_avg

# Audio transcription function
def extract_audio_text(video_path):
    audio_text = audio_model.transcribe(video_path)["text"]
    return audio_text

# Generate structured summary
def generate_structured_summary(video_embeddings, audio_text):
    visual_summary = "Key scenes include interactions and events centered around key objects or people."
    audio_summary = audio_text[:200]
    return f"Visual Summary: {visual_summary}\nAudio Summary: {audio_summary}"

# Generate answer based on question
def generate_response(structured_summary, question):
    prompt = f"{structured_summary}\n\nQuestion: {question}\nAnswer:"
    input_ids = gpt_tokenizer(prompt, return_tensors="pt").input_ids

    outputs = gpt_model.generate(input_ids=input_ids, max_new_tokens=50)
    answer_text = gpt_tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer_text

# API Endpoint
@app.route('/ask', methods=['POST'])
def ask():
    video = request.files['video']
    question = request.form.get('question')

    video_path = "ask.mp4"
    video.save(video_path)

    video_embeddings = extract_video_features(video_path)
    audio_text = extract_audio_text(video_path)
    structured_summary = generate_structured_summary(video_embeddings, audio_text)
    answer = generate_response(structured_summary, question)

    return jsonify({"answer": answer})

if __name__ == '__main__':
    app.run(debug=True)