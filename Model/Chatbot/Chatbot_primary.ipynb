{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "68ec12fe19b34337b0c3529941669a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a04c2c84c554af0830afa1b718d68ac",
              "IPY_MODEL_b5312e5ae2b64adfb03378a5a14f7c8c",
              "IPY_MODEL_e6a63c1c8f8e445faa4e46521858aa4b"
            ],
            "layout": "IPY_MODEL_76a6fddf09764df5919a4427e3900750"
          }
        },
        "2a04c2c84c554af0830afa1b718d68ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3be5c90d182d475884bcf436e7ee82e6",
            "placeholder": "​",
            "style": "IPY_MODEL_7680a7a1b8bc4c7eba03ffe472ca90c6",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "b5312e5ae2b64adfb03378a5a14f7c8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b4c34f84be434f8db7da99519a9085",
            "max": 163,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c570c744ea440798042b747162c7ec2",
            "value": 163
          }
        },
        "e6a63c1c8f8e445faa4e46521858aa4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a71cc85ea44392bda231baed9b6ee3",
            "placeholder": "​",
            "style": "IPY_MODEL_68fcc7e960ca4adba4bfcaf18c8c8744",
            "value": " 163/163 [00:00&lt;00:00, 3.98kB/s]"
          }
        },
        "76a6fddf09764df5919a4427e3900750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3be5c90d182d475884bcf436e7ee82e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7680a7a1b8bc4c7eba03ffe472ca90c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "47b4c34f84be434f8db7da99519a9085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c570c744ea440798042b747162c7ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d1a71cc85ea44392bda231baed9b6ee3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68fcc7e960ca4adba4bfcaf18c8c8744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ac4581202a04f56a6350ec85d57d25b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_612deeb8bc9f4d25ad9108e975cfaeeb",
              "IPY_MODEL_67ce78d831704579beda926e6fa30c6f",
              "IPY_MODEL_418e1df9b0c6423589d8d7b22dc889b1"
            ],
            "layout": "IPY_MODEL_5bb0a58fa38a44f8a4076cc024cd4994"
          }
        },
        "612deeb8bc9f4d25ad9108e975cfaeeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7df24746b6a4f18886283201294c148",
            "placeholder": "​",
            "style": "IPY_MODEL_a9990b13783045918f9abb06c06fc307",
            "value": "vocab.json: 100%"
          }
        },
        "67ce78d831704579beda926e6fa30c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e97a06417c3e4319a918fcc55abd7abe",
            "max": 291,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ae1952edb25a4dc6b7c1649c4a784883",
            "value": 291
          }
        },
        "418e1df9b0c6423589d8d7b22dc889b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8815997158a4e78a65a53857c343c6f",
            "placeholder": "​",
            "style": "IPY_MODEL_03f058457c3844c7b5db9d2466e4634f",
            "value": " 291/291 [00:00&lt;00:00, 5.85kB/s]"
          }
        },
        "5bb0a58fa38a44f8a4076cc024cd4994": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7df24746b6a4f18886283201294c148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9990b13783045918f9abb06c06fc307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e97a06417c3e4319a918fcc55abd7abe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae1952edb25a4dc6b7c1649c4a784883": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8815997158a4e78a65a53857c343c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03f058457c3844c7b5db9d2466e4634f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e94c23bfd5d045158e0e896b465ec6f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_143f43473038453b9030a32bf3f73cff",
              "IPY_MODEL_023ba5f7e0d94d93a31d54bbfd4ac68d",
              "IPY_MODEL_8efa68da326c4e81b403017925aaa7e0"
            ],
            "layout": "IPY_MODEL_cb231a9e734a4547bf5ed729c0b59e2f"
          }
        },
        "143f43473038453b9030a32bf3f73cff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0663c5c8cd264fdea05b5fc49bbf0093",
            "placeholder": "​",
            "style": "IPY_MODEL_750b9faa43784601ad2f096dbfac3dd1",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "023ba5f7e0d94d93a31d54bbfd4ac68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9854419d7fbd4f429982a6633ba67c1b",
            "max": 85,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a484c5239ecc48e88f5f478d49979360",
            "value": 85
          }
        },
        "8efa68da326c4e81b403017925aaa7e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_939a9bdbab5e4daca104c605047361c6",
            "placeholder": "​",
            "style": "IPY_MODEL_45be16d95d6048288cc92eccadd7c9dd",
            "value": " 85.0/85.0 [00:00&lt;00:00, 1.57kB/s]"
          }
        },
        "cb231a9e734a4547bf5ed729c0b59e2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0663c5c8cd264fdea05b5fc49bbf0093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "750b9faa43784601ad2f096dbfac3dd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9854419d7fbd4f429982a6633ba67c1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a484c5239ecc48e88f5f478d49979360": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "939a9bdbab5e4daca104c605047361c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45be16d95d6048288cc92eccadd7c9dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "574658727fd44bb39c79dfdc09f40c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2de5cde7fbf34c3e9c96cd52e7185623",
              "IPY_MODEL_9352f7f04ead406489548e0cf32eb9c9",
              "IPY_MODEL_85d44cb92f7e472394de335a2e12fb1f"
            ],
            "layout": "IPY_MODEL_1e25076983474ea38c4d8f2f68ce152a"
          }
        },
        "2de5cde7fbf34c3e9c96cd52e7185623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd6fb2f03261461e8022e133b7afb069",
            "placeholder": "​",
            "style": "IPY_MODEL_5c9b4e915ffc43908b3470ef14431a6f",
            "value": "config.json: 100%"
          }
        },
        "9352f7f04ead406489548e0cf32eb9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9455842fdeab4775b2eb84373efe9db6",
            "max": 843,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8832401ac1764d2398203938277f0a55",
            "value": 843
          }
        },
        "85d44cb92f7e472394de335a2e12fb1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9386d70990f4ca987bf88860ff737fa",
            "placeholder": "​",
            "style": "IPY_MODEL_99879d46319c47a3b639916640f2d9d8",
            "value": " 843/843 [00:00&lt;00:00, 12.4kB/s]"
          }
        },
        "1e25076983474ea38c4d8f2f68ce152a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd6fb2f03261461e8022e133b7afb069": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9b4e915ffc43908b3470ef14431a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9455842fdeab4775b2eb84373efe9db6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8832401ac1764d2398203938277f0a55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9386d70990f4ca987bf88860ff737fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99879d46319c47a3b639916640f2d9d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee4339e3946e4e1cbfe8895adb813aa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76061c2018f14daf987eb4ef2be5533a",
              "IPY_MODEL_d6a7b7aa559f47d4b5997d18021afb18",
              "IPY_MODEL_42bda8d1c78f4fe3ae8fea727ab8318b"
            ],
            "layout": "IPY_MODEL_33d8c16cceef4815a34759e24c8178bb"
          }
        },
        "76061c2018f14daf987eb4ef2be5533a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28620034689b4b35aa9ad35005e2b1de",
            "placeholder": "​",
            "style": "IPY_MODEL_19c4dd3276ad4c4db6ca434dc511ed81",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "d6a7b7aa559f47d4b5997d18021afb18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71563937a8ab45feaec93caedfd8be60",
            "max": 1262009187,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b0a96cc5f6c248689ba34d102264359f",
            "value": 1262009187
          }
        },
        "42bda8d1c78f4fe3ae8fea727ab8318b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81a3118354ab4b96b0ece665e25dd1a9",
            "placeholder": "​",
            "style": "IPY_MODEL_0c29ec2febe044ea8dde5dc03e9e7469",
            "value": " 1.26G/1.26G [00:17&lt;00:00, 92.3MB/s]"
          }
        },
        "33d8c16cceef4815a34759e24c8178bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28620034689b4b35aa9ad35005e2b1de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19c4dd3276ad4c4db6ca434dc511ed81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71563937a8ab45feaec93caedfd8be60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a96cc5f6c248689ba34d102264359f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81a3118354ab4b96b0ece665e25dd1a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c29ec2febe044ea8dde5dc03e9e7469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Using Whisper in a commercial SaaS product requires careful consideration of licensing and potential limitations. Here's what you should know:\n",
        "\n",
        "Licensing Status:\n",
        "\n",
        "\n",
        "Whisper is released under the MIT License\n",
        "This is a permissive license that allows commercial use\n",
        "You can use, modify, and distribute it in your SaaS product\n",
        "\n",
        "\n",
        "Potential Concerns:\n",
        "\n",
        "\n",
        "While the license is permissive, the model depends on PyTorch which has its own license and terms\n",
        "Resource costs may scale significantly with usage\n",
        "Running Whisper locally means you're responsible for model updates and maintenance\n",
        "No official support from OpenAI for production deployments\n",
        "\n",
        "\n",
        "Alternatives to Consider:\n",
        "\n",
        "\n",
        "Cloud Speech-to-Text services:\n",
        "\n",
        "AWS Transcribe\n",
        "Google Cloud Speech-to-Text\n",
        "Azure Speech Services\n",
        "\n",
        "\n",
        "These services provide:\n",
        "\n",
        "SLAs for uptime\n",
        "Official support\n",
        "Managed infrastructure\n",
        "Regular updates\n",
        "Better scalability\n",
        "\n",
        "\n",
        "Recommendations:\n",
        "\n",
        "\n",
        "For early stage/small scale: Whisper is fine\n",
        "For scaling up consider:\n",
        "\n",
        "Implementing a hybrid approach (Whisper + cloud backup)\n",
        "Gradually transitioning to cloud services\n",
        "Running internal benchmarks to compare accuracy and costs\n",
        "\n",
        "The main risk isn't legal but rather technical and operational. As you scale, managing your own speech recognition infrastructure might become more complex and costly than using managed services.\n",
        "Would you like me to elaborate on any of these points or discuss specific cloud alternatives in more detail?"
      ],
      "metadata": {
        "id": "zTBhLoK-KyYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from moviepy.editor import VideoFileClip\n",
        "from pydub import AudioSegment\n",
        "from pydub.playback import play\n",
        "\n",
        "# Step 1: Extract Audio from Video\n",
        "def preprocess_video(video_path, output_dir=\"processed_video\"):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    audio_path = os.path.join(output_dir, \"audio.wav\")\n",
        "\n",
        "    video = VideoFileClip(video_path)\n",
        "    video.audio.write_audiofile(audio_path, codec=\"pcm_s16le\")\n",
        "    return audio_path\n",
        "\n",
        "# Step 2: Transcribe Audio\n",
        "def transcribe_audio(audio_path):\n",
        "    from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n",
        "\n",
        "    tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "    model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
        "\n",
        "    audio = AudioSegment.from_file(audio_path).set_frame_rate(16000).set_channels(1)\n",
        "    samples = np.array(audio.get_array_of_samples(), dtype=np.float32) / 32768.0\n",
        "    inputs = tokenizer(samples, return_tensors=\"pt\", padding=\"longest\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = tokenizer.decode(predicted_ids[0])\n",
        "    return transcription\n",
        "\n",
        "# Step 3: Create Knowledge Base\n",
        "def create_knowledge_base(transcription):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "    def embed_text(text):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            embeddings = model.encoder(**inputs).last_hidden_state.mean(dim=1).squeeze()\n",
        "        return embeddings.cpu().numpy()\n",
        "\n",
        "    segments = transcription.split(\". \")\n",
        "    embeddings = [embed_text(segment) for segment in segments]\n",
        "\n",
        "    dimension = embeddings[0].shape[0]\n",
        "    faiss_index = faiss.IndexFlatL2(dimension)\n",
        "    faiss_index.add(np.array(embeddings, dtype=np.float32))\n",
        "\n",
        "    return faiss_index, segments\n",
        "\n",
        "# Step 4: Chatbot Response\n",
        "def chatbot_response(query, faiss_index, segments, tokenizer, model):\n",
        "    def embed_text(text):\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            embeddings = model.encoder(**inputs).last_hidden_state.mean(dim=1).squeeze()\n",
        "        return embeddings.cpu().numpy()\n",
        "\n",
        "    query_embedding = np.array([embed_text(query)], dtype=np.float32)\n",
        "    distances, indices = faiss_index.search(query_embedding, k=1)\n",
        "    relevant_segment = segments[indices[0][0]]\n",
        "\n",
        "    input_text = f\"Question: {query} Context: {relevant_segment}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    outputs = model.generate(**inputs, max_length=50, num_beams=3, early_stopping=True)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return response\n",
        "\n",
        "# Testing Pipeline\n",
        "def test_pipeline(video_path, questions):\n",
        "    output_dir = \"processed_video\"\n",
        "\n",
        "    print(\"Preprocessing video...\")\n",
        "    audio_path = preprocess_video(video_path, output_dir)\n",
        "\n",
        "    print(\"Transcribing audio...\")\n",
        "    transcription = transcribe_audio(audio_path)\n",
        "    print(f\"Transcription: {transcription[:200]}...\")  # Debugging snippet\n",
        "\n",
        "    print(\"Creating knowledge base...\")\n",
        "    faiss_index, segments = create_knowledge_base(transcription)\n",
        "    print(f\"Segments: {segments[:3]}\")  # Debugging snippet\n",
        "\n",
        "    print(\"Loading conversational model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "    print(\"Testing chatbot...\")\n",
        "    for question in questions:\n",
        "        response = chatbot_response(question, faiss_index, segments, tokenizer, model)\n",
        "        print(f\"Q: {question}\")\n",
        "        print(f\"A: {response}\")\n",
        "\n",
        "    shutil.rmtree(output_dir)\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    sample_video = \"/content/videoplayback.mp4\"  # Replace with your video file\n",
        "    sample_questions = [\n",
        "        \"What is the video about?\",\n",
        "        \"Can you summarize the content?\",\n",
        "        \"What are the main points discussed?\",\n",
        "        \"What is she having for lunch?\",\n",
        "        \"Where is she going for lunch?\"\n",
        "    ]\n",
        "\n",
        "    test_pipeline(sample_video, sample_questions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718,
          "referenced_widgets": [
            "68ec12fe19b34337b0c3529941669a8a",
            "2a04c2c84c554af0830afa1b718d68ac",
            "b5312e5ae2b64adfb03378a5a14f7c8c",
            "e6a63c1c8f8e445faa4e46521858aa4b",
            "76a6fddf09764df5919a4427e3900750",
            "3be5c90d182d475884bcf436e7ee82e6",
            "7680a7a1b8bc4c7eba03ffe472ca90c6",
            "47b4c34f84be434f8db7da99519a9085",
            "2c570c744ea440798042b747162c7ec2",
            "d1a71cc85ea44392bda231baed9b6ee3",
            "68fcc7e960ca4adba4bfcaf18c8c8744",
            "5ac4581202a04f56a6350ec85d57d25b",
            "612deeb8bc9f4d25ad9108e975cfaeeb",
            "67ce78d831704579beda926e6fa30c6f",
            "418e1df9b0c6423589d8d7b22dc889b1",
            "5bb0a58fa38a44f8a4076cc024cd4994",
            "c7df24746b6a4f18886283201294c148",
            "a9990b13783045918f9abb06c06fc307",
            "e97a06417c3e4319a918fcc55abd7abe",
            "ae1952edb25a4dc6b7c1649c4a784883",
            "b8815997158a4e78a65a53857c343c6f",
            "03f058457c3844c7b5db9d2466e4634f",
            "e94c23bfd5d045158e0e896b465ec6f1",
            "143f43473038453b9030a32bf3f73cff",
            "023ba5f7e0d94d93a31d54bbfd4ac68d",
            "8efa68da326c4e81b403017925aaa7e0",
            "cb231a9e734a4547bf5ed729c0b59e2f",
            "0663c5c8cd264fdea05b5fc49bbf0093",
            "750b9faa43784601ad2f096dbfac3dd1",
            "9854419d7fbd4f429982a6633ba67c1b",
            "a484c5239ecc48e88f5f478d49979360",
            "939a9bdbab5e4daca104c605047361c6",
            "45be16d95d6048288cc92eccadd7c9dd",
            "574658727fd44bb39c79dfdc09f40c39",
            "2de5cde7fbf34c3e9c96cd52e7185623",
            "9352f7f04ead406489548e0cf32eb9c9",
            "85d44cb92f7e472394de335a2e12fb1f",
            "1e25076983474ea38c4d8f2f68ce152a",
            "fd6fb2f03261461e8022e133b7afb069",
            "5c9b4e915ffc43908b3470ef14431a6f",
            "9455842fdeab4775b2eb84373efe9db6",
            "8832401ac1764d2398203938277f0a55",
            "a9386d70990f4ca987bf88860ff737fa",
            "99879d46319c47a3b639916640f2d9d8",
            "ee4339e3946e4e1cbfe8895adb813aa0",
            "76061c2018f14daf987eb4ef2be5533a",
            "d6a7b7aa559f47d4b5997d18021afb18",
            "42bda8d1c78f4fe3ae8fea727ab8318b",
            "33d8c16cceef4815a34759e24c8178bb",
            "28620034689b4b35aa9ad35005e2b1de",
            "19c4dd3276ad4c4db6ca434dc511ed81",
            "71563937a8ab45feaec93caedfd8be60",
            "b0a96cc5f6c248689ba34d102264359f",
            "81a3118354ab4b96b0ece665e25dd1a9",
            "0c29ec2febe044ea8dde5dc03e9e7469"
          ]
        },
        "id": "IRddSQKvYyUu",
        "outputId": "2101571f-5184-4af5-b42a-74221a84ca8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing video...\n",
            "MoviePy - Writing audio in processed_video/audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Transcribing audio...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68ec12fe19b34337b0c3529941669a8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ac4581202a04f56a6350ec85d57d25b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e94c23bfd5d045158e0e896b465ec6f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/843 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "574658727fd44bb39c79dfdc09f40c39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
            "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:720: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
            "  warnings.warn(\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee4339e3946e4e1cbfe8895adb813aa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription: I JUST GOT OUT OF A TWO HOUR LECTURE SO I'M HEADED TO MY SWARDY HOUSE TO GET SOME LUNCH AS WE KNOW I HAVE BEEN SICK SO I NEED TO MAKE SURE I'M EATING WITH MY MEDICINE I THINK TODAY IS MEATBALL SUVES B...\n",
            "Creating knowledge base...\n",
            "Segments: [\"I JUST GOT OUT OF A TWO HOUR LECTURE SO I'M HEADED TO MY SWARDY HOUSE TO GET SOME LUNCH AS WE KNOW I HAVE BEEN SICK SO I NEED TO MAKE SURE I'M EATING WITH MY MEDICINE I THINK TODAY IS MEATBALL SUVES BUT IM MUSGOINTO GET NEATBALLS GOES OF MY THEAVER PARTS LET'S GO TOGETHER  ME MEDY DEDECORATIONS ARE GORGEOUS MY UNCLE IS IN MY RIGHT TO KNOW IT I HAVE SO MANY SOTS TO PICKER  CAN I JOIN YOU GUY THINK   MA LIITS WRIM LI LOOKING TAN MAIN JUST KINNYAN KEEP ALL ALLI UTTEY NEED TO GO GET A SOWED I  WE GOT NECOLS MOFFLES RINE AND WATER MIND TIS I MEDIAT ABOUT I YE  MINDIS TINKIN E ELA I THINK THAT'S RIGHT I JUST APPLIED FOR GRADUATION ALL DONE JA DEA I HAVE TO GO DO HOMEARCH SHAN AN I ARE WALKING BACK HOME NOW SO I'LL SEE AGUIS LATER BAGIES\"]\n",
            "Loading conversational model...\n",
            "Testing chatbot...\n",
            "Q: What is the video about?\n",
            "A: I'M HEADED TO MY SWARDY HOUSE TO GET SOME LUNCH AS WE KNOW I HAVE BEEN SICK SO I NEED TO MAKE SURE I'M EATING WITH M\n",
            "Q: Can you summarize the content?\n",
            "A: I'M HEADED TO MY SWARDY HOUSE TO GET SOME LUNCH AS WE KNOW I HAVE BEEN SICK SO I NEED TO MAKE SURE I'M EATING WITH M\n",
            "Q: What are the main points discussed?\n",
            "A: I'M HEADED TO MY SWARDY HOUSE TO GET SOME LUNCH AS WE KNOW I HAVE BEEN SICK SO I NEED TO MAKE SURE I'M EATING WITH M\n",
            "Q: What is she having for lunch?\n",
            "A: a salad\n",
            "Q: Where is she going for lunch?\n",
            "A: a restaurant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYAlWiQpa7Cp",
        "outputId": "3b94a619-4a8a-45bf-e52b-0be12906c18c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Working**"
      ],
      "metadata": {
        "id": "RxsLFiMZdi9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import warnings\n",
        "from typing import List, Dict, Union, Optional\n",
        "from transformers import pipeline\n",
        "import whisper\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from moviepy.editor import VideoFileClip\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "\n",
        "class FixedVideoChatbot:\n",
        "    def __init__(self, cache_dir: str = \"video_cache\", log_file: str = \"chatbot.log\"):\n",
        "        self.setup_logging(log_file)\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "            self.transcription_model = whisper.load_model(\"base\", device=self.device)\n",
        "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            self.qa_pipeline = pipeline(\n",
        "                \"question-answering\",\n",
        "                model=\"deepset/roberta-base-squad2\",\n",
        "                device=0 if torch.cuda.is_available() else -1\n",
        "            )\n",
        "            self.logger.info(\"Models loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        self.cached_data = {}\n",
        "\n",
        "    def setup_logging(self, log_file: str):\n",
        "        self.logger = logging.getLogger('VideoChatbot')\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_handler.setFormatter(formatter)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def extract_audio(self, video_path: str) -> np.ndarray:\n",
        "        \"\"\"Extract audio using librosa instead of moviepy.\"\"\"\n",
        "        try:\n",
        "            # Extract audio using librosa\n",
        "            self.logger.info(f\"Extracting audio from {video_path}\")\n",
        "            audio_array, sr = librosa.load(video_path, sr=16000, mono=True)\n",
        "\n",
        "            # Ensure audio is the correct format for whisper\n",
        "            if not isinstance(audio_array, np.ndarray):\n",
        "                raise ValueError(\"Audio extraction failed\")\n",
        "\n",
        "            if len(audio_array.shape) != 1:\n",
        "                audio_array = audio_array.mean(axis=-1)\n",
        "\n",
        "            return audio_array.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in audio extraction: {str(e)}\")\n",
        "            try:\n",
        "                # Fallback method using moviepy\n",
        "                self.logger.info(\"Attempting fallback audio extraction\")\n",
        "                with VideoFileClip(video_path) as video:\n",
        "                    if video.audio is None:\n",
        "                        raise ValueError(\"Video has no audio track\")\n",
        "                    audio_array = video.audio.to_soundarray(fps=16000)\n",
        "                    if len(audio_array.shape) > 1:\n",
        "                        audio_array = audio_array.mean(axis=1)\n",
        "                    return audio_array.astype(np.float32)\n",
        "            except Exception as fallback_error:\n",
        "                self.logger.error(f\"Fallback audio extraction failed: {str(fallback_error)}\")\n",
        "                raise\n",
        "\n",
        "    def process_chunks(self, audio_array: np.ndarray, chunk_duration: int = 30) -> List[Dict]:\n",
        "        \"\"\"Process audio in chunks to handle memory constraints.\"\"\"\n",
        "        chunk_size = chunk_duration * 16000  # assuming 16kHz sample rate\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(audio_array), chunk_size):\n",
        "            chunk = audio_array[i:i + chunk_size]\n",
        "            if len(chunk) < 100:  # Skip very small chunks\n",
        "                continue\n",
        "\n",
        "            # Ensure chunk is 1D array\n",
        "            if len(chunk.shape) > 1:\n",
        "                chunk = chunk.mean(axis=-1)\n",
        "\n",
        "            result = self.transcription_model.transcribe(chunk)\n",
        "\n",
        "            # Adjust timestamps\n",
        "            for segment in result[\"segments\"]:\n",
        "                segment[\"start\"] += i / 16000  # Convert samples to seconds\n",
        "                segment[\"end\"] += i / 16000\n",
        "                chunks.append(segment)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_video(self, video_path: str) -> Dict:\n",
        "        \"\"\"Process video with improved error handling.\"\"\"\n",
        "        try:\n",
        "            video_id = os.path.basename(video_path)\n",
        "            cache_file = os.path.join(self.cache_dir, f\"{video_id}_transcription.json\")\n",
        "\n",
        "            # Check cache\n",
        "            if os.path.exists(cache_file):\n",
        "                with open(cache_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "\n",
        "            # Extract and process audio\n",
        "            audio_array = self.extract_audio(video_path)\n",
        "\n",
        "            # Process in chunks\n",
        "            transcription_segments = self.process_chunks(audio_array)\n",
        "\n",
        "            # Combine results\n",
        "            transcription_data = {\n",
        "                \"full_text\": \" \".join(seg[\"text\"] for seg in transcription_segments),\n",
        "                \"segments\": transcription_segments\n",
        "            }\n",
        "\n",
        "            # Cache results\n",
        "            with open(cache_file, 'w') as f:\n",
        "                json.dump(transcription_data, f)\n",
        "\n",
        "            return transcription_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing video {video_path}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def find_relevant_context(self, query: str, transcription_data: Dict) -> str:\n",
        "        \"\"\"Find relevant context with improved error handling.\"\"\"\n",
        "        try:\n",
        "            if not transcription_data[\"segments\"]:\n",
        "                return \"\"\n",
        "\n",
        "            # Convert query to embedding\n",
        "            query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "            # Get segment texts and embeddings\n",
        "            segment_texts = [s[\"text\"] for s in transcription_data[\"segments\"]]\n",
        "            if not segment_texts:\n",
        "                return \"\"\n",
        "\n",
        "            segment_embeddings = self.embedding_model.encode(segment_texts)\n",
        "\n",
        "            # Ensure proper shapes\n",
        "            if len(query_embedding.shape) == 1:\n",
        "                query_embedding = query_embedding.reshape(1, -1)\n",
        "            if len(segment_embeddings.shape) == 1:\n",
        "                segment_embeddings = segment_embeddings.reshape(1, -1)\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity(query_embedding, segment_embeddings)[0]\n",
        "\n",
        "            # Get top segments\n",
        "            top_k = min(3, len(similarities))\n",
        "            top_indices = np.argsort(similarities)[-top_k:]\n",
        "\n",
        "            # Combine context\n",
        "            context = \" \".join(segment_texts[i] for i in top_indices)\n",
        "            return context\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error finding context: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def get_response(self, query: str, video_paths: Union[str, List[str]]) -> Dict:\n",
        "        \"\"\"Generate response with improved error handling.\"\"\"\n",
        "        try:\n",
        "            if isinstance(video_paths, str):\n",
        "                video_paths = [video_paths]\n",
        "\n",
        "            all_contexts = []\n",
        "            for video_path in video_paths:\n",
        "                if video_path not in self.cached_data:\n",
        "                    self.cached_data[video_path] = self.process_video(video_path)\n",
        "\n",
        "                context = self.find_relevant_context(query, self.cached_data[video_path])\n",
        "                if context:\n",
        "                    all_contexts.append(context)\n",
        "\n",
        "            if not all_contexts:\n",
        "                return {\n",
        "                    \"answer\": \"No relevant information found in the videos.\",\n",
        "                    \"confidence\": 0.0,\n",
        "                    \"context\": \"\"\n",
        "                }\n",
        "\n",
        "            combined_context = \" \".join(all_contexts)\n",
        "\n",
        "            # Generate answer\n",
        "            answer = self.qa_pipeline(\n",
        "                question=query,\n",
        "                context=combined_context,\n",
        "                max_answer_length=100\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer[\"answer\"],\n",
        "                \"confidence\": answer[\"score\"],\n",
        "                \"context\": combined_context\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating response: {str(e)}\")\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"answer\": \"Sorry, I encountered an error processing your question.\",\n",
        "                \"confidence\": 0.0\n",
        "            }\n",
        "\n",
        "def main():\n",
        "    # Example usage\n",
        "    chatbot = FixedVideoChatbot()\n",
        "\n",
        "    video_path = \"/content/videoplayback.mp4\"  # Your video path\n",
        "\n",
        "    questions = [\n",
        "        \"What is the main topic discussed in the video?\",\n",
        "        \"What are the key points mentioned?\",\n",
        "        \"Can you summarize the conclusions?\",\n",
        "        \"what she is having in lunch?\",\n",
        "        \"where she is going for lunch?\"\n",
        "    ]\n",
        "\n",
        "    for question in questions:\n",
        "        try:\n",
        "            response = chatbot.get_response(question, video_path)\n",
        "            print(f\"\\nQuestion: {question}\")\n",
        "            print(f\"Answer: {response['answer']}\")\n",
        "            print(f\"Confidence: {response['confidence']:.2f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq8kOPCRdCTQ",
        "outputId": "c4f5b390-08ed-44ab-84d5-0d3d3508e1dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:VideoChatbot:Using device: cpu\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "\n",
            "Device set to use cpu\n",
            "INFO:VideoChatbot:Models loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is the main topic discussed in the video?\n",
            "Answer: media\n",
            "Confidence: 0.09\n",
            "\n",
            "Question: What are the key points mentioned?\n",
            "Answer: media stuff\n",
            "Confidence: 0.02\n",
            "\n",
            "Question: Can you summarize the conclusions?\n",
            "Answer: Bye guys.  I have to go do homework.\n",
            "Confidence: 0.00\n",
            "\n",
            "Question: what she is having in lunch?\n",
            "Answer: meatballs, baffle fries, and watermelon\n",
            "Confidence: 0.33\n",
            "\n",
            "Question: where she is going for lunch?\n",
            "Answer: my swarty house\n",
            "Confidence: 0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import warnings\n",
        "from typing import List, Dict, Union, Optional\n",
        "from transformers import pipeline\n",
        "import whisper\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from moviepy.editor import VideoFileClip\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "\n",
        "class FixedVideoChatbot:\n",
        "    def __init__(self, cache_dir: str = \"video_cache\", log_file: str = \"chatbot.log\"):\n",
        "        self.setup_logging(log_file)\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "            self.transcription_model = whisper.load_model(\"base\", device=self.device)\n",
        "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            self.qa_pipeline = pipeline(\n",
        "                \"question-answering\",\n",
        "                model=\"deepset/roberta-base-squad2\",\n",
        "                device=0 if torch.cuda.is_available() else -1\n",
        "            )\n",
        "            self.logger.info(\"Models loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        self.cached_data = {}\n",
        "\n",
        "    def setup_logging(self, log_file: str):\n",
        "        self.logger = logging.getLogger('VideoChatbot')\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_handler.setFormatter(formatter)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def extract_audio(self, video_path: str) -> np.ndarray:\n",
        "        \"\"\"Extract audio using librosa instead of moviepy.\"\"\"\n",
        "        try:\n",
        "            # Extract audio using librosa\n",
        "            self.logger.info(f\"Extracting audio from {video_path}\")\n",
        "            audio_array, sr = librosa.load(video_path, sr=16000, mono=True)\n",
        "\n",
        "            # Ensure audio is the correct format for whisper\n",
        "            if not isinstance(audio_array, np.ndarray):\n",
        "                raise ValueError(\"Audio extraction failed\")\n",
        "\n",
        "            if len(audio_array.shape) != 1:\n",
        "                audio_array = audio_array.mean(axis=-1)\n",
        "\n",
        "            return audio_array.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in audio extraction: {str(e)}\")\n",
        "            try:\n",
        "                # Fallback method using moviepy\n",
        "                self.logger.info(\"Attempting fallback audio extraction\")\n",
        "                with VideoFileClip(video_path) as video:\n",
        "                    if video.audio is None:\n",
        "                        raise ValueError(\"Video has no audio track\")\n",
        "                    audio_array = video.audio.to_soundarray(fps=16000)\n",
        "                    if len(audio_array.shape) > 1:\n",
        "                        audio_array = audio_array.mean(axis=1)\n",
        "                    return audio_array.astype(np.float32)\n",
        "            except Exception as fallback_error:\n",
        "                self.logger.error(f\"Fallback audio extraction failed: {str(fallback_error)}\")\n",
        "                raise\n",
        "\n",
        "    def process_chunks(self, audio_array: np.ndarray, chunk_duration: int = 30) -> List[Dict]:\n",
        "        \"\"\"Process audio in chunks to handle memory constraints.\"\"\"\n",
        "        chunk_size = chunk_duration * 16000  # assuming 16kHz sample rate\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(audio_array), chunk_size):\n",
        "            chunk = audio_array[i:i + chunk_size]\n",
        "            if len(chunk) < 100:  # Skip very small chunks\n",
        "                continue\n",
        "\n",
        "            # Ensure chunk is 1D array\n",
        "            if len(chunk.shape) > 1:\n",
        "                chunk = chunk.mean(axis=-1)\n",
        "\n",
        "            result = self.transcription_model.transcribe(chunk)\n",
        "\n",
        "            # Adjust timestamps\n",
        "            for segment in result[\"segments\"]:\n",
        "                segment[\"start\"] += i / 16000  # Convert samples to seconds\n",
        "                segment[\"end\"] += i / 16000\n",
        "                chunks.append(segment)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_video(self, video_path: str) -> Dict:\n",
        "        \"\"\"Process video with improved error handling.\"\"\"\n",
        "        try:\n",
        "            video_id = os.path.basename(video_path)\n",
        "            cache_file = os.path.join(self.cache_dir, f\"{video_id}_transcription.json\")\n",
        "\n",
        "            # Check cache\n",
        "            if os.path.exists(cache_file):\n",
        "                with open(cache_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "\n",
        "            # Extract and process audio\n",
        "            audio_array = self.extract_audio(video_path)\n",
        "\n",
        "            # Process in chunks\n",
        "            transcription_segments = self.process_chunks(audio_array)\n",
        "\n",
        "            # Combine results\n",
        "            transcription_data = {\n",
        "                \"full_text\": \" \".join(seg[\"text\"] for seg in transcription_segments),\n",
        "                \"segments\": transcription_segments\n",
        "            }\n",
        "\n",
        "            # Cache results\n",
        "            with open(cache_file, 'w') as f:\n",
        "                json.dump(transcription_data, f)\n",
        "\n",
        "            return transcription_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing video {video_path}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def find_relevant_context(self, query: str, transcription_data: Dict) -> str:\n",
        "        \"\"\"Find relevant context with improved error handling.\"\"\"\n",
        "        try:\n",
        "            if not transcription_data[\"segments\"]:\n",
        "                return \"\"\n",
        "\n",
        "            # Convert query to embedding\n",
        "            query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "            # Get segment texts and embeddings\n",
        "            segment_texts = [s[\"text\"] for s in transcription_data[\"segments\"]]\n",
        "            if not segment_texts:\n",
        "                return \"\"\n",
        "\n",
        "            segment_embeddings = self.embedding_model.encode(segment_texts)\n",
        "\n",
        "            # Ensure proper shapes\n",
        "            if len(query_embedding.shape) == 1:\n",
        "                query_embedding = query_embedding.reshape(1, -1)\n",
        "            if len(segment_embeddings.shape) == 1:\n",
        "                segment_embeddings = segment_embeddings.reshape(1, -1)\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity(query_embedding, segment_embeddings)[0]\n",
        "\n",
        "            # Get top segments\n",
        "            top_k = min(3, len(similarities))\n",
        "            top_indices = np.argsort(similarities)[-top_k:]\n",
        "\n",
        "            # Combine context\n",
        "            context = \" \".join(segment_texts[i] for i in top_indices)\n",
        "            return context\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error finding context: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def get_response(self, query: str, video_paths: Union[str, List[str]]) -> Dict:\n",
        "        \"\"\"Generate response with improved error handling.\"\"\"\n",
        "        try:\n",
        "            if isinstance(video_paths, str):\n",
        "                video_paths = [video_paths]\n",
        "\n",
        "            all_contexts = []\n",
        "            for video_path in video_paths:\n",
        "                if video_path not in self.cached_data:\n",
        "                    self.cached_data[video_path] = self.process_video(video_path)\n",
        "\n",
        "                context = self.find_relevant_context(query, self.cached_data[video_path])\n",
        "                if context:\n",
        "                    all_contexts.append(context)\n",
        "\n",
        "            if not all_contexts:\n",
        "                return {\n",
        "                    \"answer\": \"No relevant information found in the videos.\",\n",
        "                    \"confidence\": 0.0,\n",
        "                    \"context\": \"\"\n",
        "                }\n",
        "\n",
        "            combined_context = \" \".join(all_contexts)\n",
        "\n",
        "            # Generate answer\n",
        "            answer = self.qa_pipeline(\n",
        "                question=query,\n",
        "                context=combined_context,\n",
        "                max_answer_length=100\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer[\"answer\"],\n",
        "                \"confidence\": answer[\"score\"],\n",
        "                \"context\": combined_context\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating response: {str(e)}\")\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"answer\": \"Sorry, I encountered an error processing your question.\",\n",
        "                \"confidence\": 0.0\n",
        "            }\n",
        "\n",
        "def main():\n",
        "    # Example usage\n",
        "    chatbot = FixedVideoChatbot()\n",
        "\n",
        "    video_path = \"/content/test.mp4\"  # Your video path\n",
        "\n",
        "    questions = [\n",
        "        \"What is the main topic discussed in the video?\",\n",
        "        \"What are the key points mentioned?\",\n",
        "        \"who is she?\",\n",
        "        \"what she is cooking?\",\n",
        "        \"why she is cooking?\"\n",
        "    ]\n",
        "\n",
        "    for question in questions:\n",
        "        try:\n",
        "            response = chatbot.get_response(question, video_path)\n",
        "            print(f\"\\nQuestion: {question}\")\n",
        "            print(f\"Answer: {response['answer']}\")\n",
        "            print(f\"Confidence: {response['confidence']:.2f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrsSKDvKdpXe",
        "outputId": "df168ff2-fbc6-4504-abf2-d2bde1bf0432"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:VideoChatbot:Using device: cpu\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "\n",
            "Device set to use cpu\n",
            "INFO:VideoChatbot:Models loaded successfully\n",
            "INFO:VideoChatbot:Extracting audio from /content/test.mp4\n",
            "WARNING:py.warnings:<ipython-input-28-f6a8927705e5>:55: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_array, sr = librosa.load(video_path, sr=16000, mono=True)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is the main topic discussed in the video?\n",
            "Answer: green flags\n",
            "Confidence: 0.00\n",
            "\n",
            "Question: What are the key points mentioned?\n",
            "Answer: respectful, funny, positive, generous, emotionally healthy, and hot\n",
            "Confidence: 0.25\n",
            "\n",
            "Question: who is she?\n",
            "Answer: single life\n",
            "Confidence: 0.04\n",
            "\n",
            "Question: what she is cooking?\n",
            "Answer: Ubucho about with four different toppings for a picnic\n",
            "Confidence: 0.03\n",
            "\n",
            "Question: why she is cooking?\n",
            "Answer: He made me want to cook for him\n",
            "Confidence: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3jndlJeJEhi",
        "outputId": "99bbfac7-e7e9-482a-ddd5-2a286d1a166c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-mw_ypv6q\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-mw_ypv6q\n",
            "  Resolved https://github.com/openai/whisper.git to commit 90db0de1896c23cbfaf0c58bc2d30665f709f170\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper==20240930)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803583 sha256=20fee669e13012f4a01f32ec8c00e63ce75f09ad1ef544d55c3afa918466b508\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-03zvuoy_/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tried image/video but not working"
      ],
      "metadata": {
        "id": "MrWjuzT5iFlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "from typing import List, Dict, Union, Tuple\n",
        "from transformers import pipeline\n",
        "import whisper\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import librosa\n",
        "import logging\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "class EnhancedVideoChatbot:\n",
        "    def __init__(self, cache_dir: str = \"video_cache\", log_file: str = \"chatbot.log\"):\n",
        "        self.setup_logging(log_file)\n",
        "        self.cache_dir = cache_dir\n",
        "        self.frame_cache_dir = os.path.join(cache_dir, \"frames\")\n",
        "        os.makedirs(self.cache_dir, exist_ok=True)\n",
        "        os.makedirs(self.frame_cache_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "            # Initialize models\n",
        "            self.transcription_model = whisper.load_model(\"base\", device=self.device)\n",
        "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            self.qa_pipeline = pipeline(\n",
        "                \"question-answering\",\n",
        "                model=\"deepset/roberta-base-squad2\",\n",
        "                device=0 if torch.cuda.is_available() else -1\n",
        "            )\n",
        "\n",
        "            self.logger.info(\"Models loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        self.cached_data = {}\n",
        "        self.video_metadata = {}\n",
        "\n",
        "    def setup_logging(self, log_file: str):\n",
        "        self.logger = logging.getLogger('VideoChatbot')\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_handler.setFormatter(formatter)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def extract_frames(self, video_path: str, segment_timestamps: List[Dict]) -> Dict[float, str]:\n",
        "        \"\"\"Extract and save frames at relevant timestamps.\"\"\"\n",
        "        try:\n",
        "            video_id = os.path.basename(video_path)\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "            frame_dict = {}\n",
        "\n",
        "            for segment in segment_timestamps:\n",
        "                start_time = segment[\"start\"]\n",
        "                frame_number = int(start_time * fps)\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                if ret:\n",
        "                    frame_path = os.path.join(\n",
        "                        self.frame_cache_dir,\n",
        "                        f\"{video_id}_frame_{start_time:.2f}.jpg\"\n",
        "                    )\n",
        "                    cv2.imwrite(frame_path, frame)\n",
        "                    frame_dict[start_time] = frame_path\n",
        "\n",
        "            cap.release()\n",
        "            return frame_dict\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error extracting frames: {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def process_video(self, video_path: str) -> Dict:\n",
        "        try:\n",
        "            video_id = os.path.basename(video_path)\n",
        "            cache_file = os.path.join(self.cache_dir, f\"{video_id}_transcription.json\")\n",
        "\n",
        "            if os.path.exists(cache_file):\n",
        "                with open(cache_file, 'r') as f:\n",
        "                    processed_data = json.load(f)\n",
        "                    if video_id not in self.video_metadata:\n",
        "                        self.video_metadata[video_id] = {\n",
        "                            \"path\": video_path,\n",
        "                            \"frames\": processed_data.get(\"frames\", {})\n",
        "                        }\n",
        "                    return processed_data\n",
        "\n",
        "            # Extract audio and transcribe\n",
        "            audio_array = self.extract_audio(video_path)\n",
        "            transcription = {\"text\": \"\", \"segments\": []}\n",
        "\n",
        "            # Process in chunks\n",
        "            max_duration = 25 * 16000  # 25 seconds chunks\n",
        "            for i in range(0, len(audio_array), max_duration):\n",
        "                chunk = audio_array[i:min(i + max_duration, len(audio_array))]\n",
        "                result = self.transcription_model.transcribe(chunk)\n",
        "\n",
        "                for segment in result[\"segments\"]:\n",
        "                    segment[\"start\"] += i / 16000\n",
        "                    segment[\"end\"] += i / 16000\n",
        "                    transcription[\"segments\"].append(segment)\n",
        "                transcription[\"text\"] += \" \" + result[\"text\"]\n",
        "\n",
        "            # Merge segments and extract frames\n",
        "            merged_segments = []\n",
        "            current_segment = {\"text\": \"\", \"start\": 0, \"end\": 0}\n",
        "\n",
        "            for segment in transcription[\"segments\"]:\n",
        "                if len(current_segment[\"text\"].split()) < 15:\n",
        "                    current_segment[\"text\"] += \" \" + segment[\"text\"]\n",
        "                    current_segment[\"end\"] = segment[\"end\"]\n",
        "                else:\n",
        "                    if current_segment[\"text\"]:\n",
        "                        merged_segments.append(current_segment.copy())\n",
        "                    current_segment = segment\n",
        "\n",
        "            if current_segment[\"text\"]:\n",
        "                merged_segments.append(current_segment)\n",
        "\n",
        "            # Extract frames for each segment\n",
        "            frame_dict = self.extract_frames(video_path, merged_segments)\n",
        "\n",
        "            transcription_data = {\n",
        "                \"full_text\": transcription[\"text\"].strip(),\n",
        "                \"segments\": merged_segments,\n",
        "                \"frames\": frame_dict\n",
        "            }\n",
        "\n",
        "            with open(cache_file, 'w') as f:\n",
        "                json.dump(transcription_data, f)\n",
        "\n",
        "            self.video_metadata[video_id] = {\n",
        "                \"path\": video_path,\n",
        "                \"frames\": frame_dict\n",
        "            }\n",
        "\n",
        "            return transcription_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing video {video_path}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def extract_audio(self, video_path: str) -> np.ndarray:\n",
        "        try:\n",
        "            audio_array, sr = librosa.load(video_path, sr=16000, mono=True)\n",
        "            return audio_array.astype(np.float32)\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in audio extraction: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def get_relevant_segments(self, query: str, transcription_data: Dict) -> List[Dict]:\n",
        "        try:\n",
        "            if not transcription_data[\"segments\"]:\n",
        "                return []\n",
        "\n",
        "            query_embedding = self.embedding_model.encode([query], convert_to_tensor=True)\n",
        "            segment_texts = [s[\"text\"] for s in transcription_data[\"segments\"]]\n",
        "            segment_embeddings = self.embedding_model.encode(segment_texts, convert_to_tensor=True)\n",
        "\n",
        "            similarities = cosine_similarity(\n",
        "                query_embedding.cpu().numpy(),\n",
        "                segment_embeddings.cpu().numpy()\n",
        "            )[0]\n",
        "\n",
        "            # Get top segments with their similarity scores\n",
        "            top_k = min(3, len(similarities))\n",
        "            top_indices = np.argsort(similarities)[-top_k:]\n",
        "\n",
        "            relevant_segments = []\n",
        "            for idx in top_indices:\n",
        "                segment = transcription_data[\"segments\"][idx].copy()\n",
        "                segment[\"similarity\"] = float(similarities[idx])\n",
        "                relevant_segments.append(segment)\n",
        "\n",
        "            return sorted(relevant_segments, key=lambda x: x[\"similarity\"], reverse=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error finding relevant segments: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def get_response(self, query: str, video_paths: Union[str, List[str]]) -> Dict:\n",
        "        try:\n",
        "            if isinstance(video_paths, str):\n",
        "                video_paths = [video_paths]\n",
        "\n",
        "            all_responses = []\n",
        "\n",
        "            for video_path in video_paths:\n",
        "                video_id = os.path.basename(video_path)\n",
        "\n",
        "                if video_path not in self.cached_data:\n",
        "                    self.cached_data[video_path] = self.process_video(video_path)\n",
        "\n",
        "                # Get relevant segments with timestamps\n",
        "                relevant_segments = self.get_relevant_segments(\n",
        "                    query,\n",
        "                    self.cached_data[video_path]\n",
        "                )\n",
        "\n",
        "                if not relevant_segments:\n",
        "                    continue\n",
        "\n",
        "                # Get frames for relevant segments\n",
        "                frames = self.cached_data[video_path].get(\"frames\", {})\n",
        "\n",
        "                for segment in relevant_segments:\n",
        "                    if segment[\"similarity\"] < 0.3:  # Similarity threshold\n",
        "                        continue\n",
        "\n",
        "                    start_time = segment[\"start\"]\n",
        "                    frame_path = None\n",
        "\n",
        "                    # Find the closest frame\n",
        "                    if frames:\n",
        "                        closest_time = min(frames.keys(), key=lambda x: abs(float(x) - start_time))\n",
        "                        frame_path = frames[closest_time]\n",
        "\n",
        "                    response = {\n",
        "                        \"video_id\": video_id,\n",
        "                        \"video_path\": video_path,\n",
        "                        \"segment_text\": segment[\"text\"],\n",
        "                        \"start_time\": start_time,\n",
        "                        \"end_time\": segment[\"end\"],\n",
        "                        \"confidence\": segment[\"similarity\"],\n",
        "                        \"frame_path\": frame_path\n",
        "                    }\n",
        "\n",
        "                    all_responses.append(response)\n",
        "\n",
        "            if not all_responses:\n",
        "                return {\n",
        "                    \"answer\": \"No relevant information found in the videos.\",\n",
        "                    \"confidence\": 0.0,\n",
        "                    \"segments\": []\n",
        "                }\n",
        "\n",
        "            # Sort responses by confidence\n",
        "            all_responses.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
        "\n",
        "            # Get the best answer using QA pipeline\n",
        "            best_context = \" \".join([r[\"segment_text\"] for r in all_responses[:2]])\n",
        "            qa_result = self.qa_pipeline(\n",
        "                question=query,\n",
        "                context=best_context,\n",
        "                max_answer_length=100,\n",
        "                handle_impossible_answer=True\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"answer\": qa_result[\"answer\"],\n",
        "                \"confidence\": qa_result[\"score\"],\n",
        "                \"segments\": all_responses  # Include all relevant segments with metadata\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating response: {str(e)}\")\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"answer\": \"Sorry, I encountered an error processing your question.\",\n",
        "                \"confidence\": 0.0,\n",
        "                \"segments\": []\n",
        "            }\n",
        "\n",
        "def main():\n",
        "    chatbot = EnhancedVideoChatbot()\n",
        "    video_paths = [\"/content/test.mp4\", \"/content/videoplayback.mp4\"]  # Example with multiple videos\n",
        "\n",
        "    questions = [\n",
        "        \"What is the main topic discussed in the video?\",\n",
        "        \"What are the key points mentioned?\",\n",
        "        \"who is she?\",\n",
        "        \"what she is making?\",\n",
        "        \"why she is cooking?\",\n",
        "        \"how swarty house looks like?\"\n",
        "    ]\n",
        "\n",
        "    print(\"Processing videos and generating responses...\")\n",
        "    for question in questions:\n",
        "        try:\n",
        "            response = chatbot.get_response(question, video_paths)\n",
        "            print(f\"\\nQuestion: {question}\")\n",
        "            print(f\"Answer: {response['answer']}\")\n",
        "            print(f\"Confidence: {response['confidence']:.2f}\")\n",
        "\n",
        "            print(\"\\nRelevant video segments:\")\n",
        "            for segment in response[\"segments\"]:\n",
        "                print(f\"\\nVideo: {segment['video_id']}\")\n",
        "                print(f\"Timestamp: {segment['start_time']:.2f}s - {segment['end_time']:.2f}s\")\n",
        "                print(f\"Text: {segment['segment_text']}\")\n",
        "                if segment['frame_path']:\n",
        "                    print(f\"Frame captured at: {segment['frame_path']}\")\n",
        "                print(f\"Confidence: {segment['confidence']:.2f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing question: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKQISbuNLf7n",
        "outputId": "b738e40b-a595-448d-beb2-22d61f1fd1e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:VideoChatbot:Using device: cpu\n",
            "Device set to use cpu\n",
            "INFO:VideoChatbot:Models loaded successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing videos and generating responses...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-ea00b24597fd>:153: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_array, sr = librosa.load(video_path, sr=16000, mono=True)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "<ipython-input-4-ea00b24597fd>:153: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_array, sr = librosa.load(video_path, sr=16000, mono=True)\n",
            "/usr/local/lib/python3.10/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Question: What is the main topic discussed in the video?\n",
            "Answer: No relevant information found in the videos.\n",
            "Confidence: 0.00\n",
            "\n",
            "Relevant video segments:\n",
            "\n",
            "Question: What are the key points mentioned?\n",
            "Answer: No relevant information found in the videos.\n",
            "Confidence: 0.00\n",
            "\n",
            "Relevant video segments:\n",
            "\n",
            "Question: who is she?\n",
            "Answer: No relevant information found in the videos.\n",
            "Confidence: 0.00\n",
            "\n",
            "Relevant video segments:\n",
            "\n",
            "Question: what she is making?\n",
            "Answer: No relevant information found in the videos.\n",
            "Confidence: 0.00\n",
            "\n",
            "Relevant video segments:\n",
            "\n",
            "Question: why she is cooking?\n",
            "Answer: packing us new butchle about with four different toppings for a picnic\n",
            "Confidence: 0.07\n",
            "\n",
            "Relevant video segments:\n",
            "\n",
            "Video: test.mp4\n",
            "Timestamp: 0.00s - 3.76s\n",
            "Text:   Hey, remember I got the ick from the guy who asked me to cook on her date?\n",
            "Frame captured at: video_cache/frames/test.mp4_frame_0.00.jpg\n",
            "Confidence: 0.47\n",
            "\n",
            "Video: test.mp4\n",
            "Timestamp: 7.16s - 14.40s\n",
            "Text:  He made me want to cook for him.  And when I say cook, I'm packing us new butchle about with four different toppings for a picnic that I planned.\n",
            "Frame captured at: video_cache/frames/test.mp4_frame_7.16.jpg\n",
            "Confidence: 0.40\n",
            "\n",
            "Question: how swarty house looks like?\n",
            "Answer: \n",
            "Confidence: 0.97\n",
            "\n",
            "Relevant video segments:\n",
            "\n",
            "Video: videoplayback.mp4\n",
            "Timestamp: 0.00s - 4.32s\n",
            "Text:   I just got out of a two hour lecture so I'm headed to my swarty house to get some lunch.\n",
            "Frame captured at: video_cache/frames/videoplayback.mp4_frame_0.00.jpg\n",
            "Confidence: 0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "from typing import List, Dict\n",
        "from transformers import pipeline\n",
        "import whisper\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import cv2\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "class VideoQA:\n",
        "    def __init__(self, output_dir: str = \"outputs\"):\n",
        "        # Create output directories\n",
        "        self.output_dir = output_dir\n",
        "        self.clips_dir = os.path.join(output_dir, \"clips\")\n",
        "        self.frames_dir = os.path.join(output_dir, \"frames\")\n",
        "        os.makedirs(self.clips_dir, exist_ok=True)\n",
        "        os.makedirs(self.frames_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize models\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.transcriber = whisper.load_model(\"base\", device=self.device)\n",
        "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.qa_model = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\", device=0 if self.device==\"cuda\" else -1)\n",
        "\n",
        "        self.video_data = {}\n",
        "\n",
        "    def process_video(self, video_path: str) -> None:\n",
        "        \"\"\"Process video and store transcription with timestamps\"\"\"\n",
        "        video_id = os.path.basename(video_path)\n",
        "\n",
        "        if video_id in self.video_data:\n",
        "            return\n",
        "\n",
        "        # Transcribe video\n",
        "        result = self.transcriber.transcribe(video_path)\n",
        "\n",
        "        # Store segments with timestamps\n",
        "        self.video_data[video_id] = {\n",
        "            \"path\": video_path,\n",
        "            \"segments\": result[\"segments\"],\n",
        "            \"full_text\": result[\"text\"]\n",
        "        }\n",
        "\n",
        "    def extract_clip(self, video_path: str, start_time: float, end_time: float) -> Dict:\n",
        "        \"\"\"Extract clip and frame from video\"\"\"\n",
        "        video_id = os.path.basename(video_path)\n",
        "\n",
        "        # Add small padding to clip\n",
        "        start_time = max(0, start_time - 1)\n",
        "        duration = end_time - start_time + 2\n",
        "\n",
        "        # Extract clip\n",
        "        clip_path = os.path.join(self.clips_dir, f\"{video_id}_{start_time:.1f}_{end_time:.1f}.mp4\")\n",
        "        if not os.path.exists(clip_path):\n",
        "            with VideoFileClip(video_path) as video:\n",
        "                clip = video.subclip(start_time, end_time)\n",
        "                clip.write_videofile(clip_path, codec='libx264', audio_codec='aac')\n",
        "\n",
        "        # Extract middle frame\n",
        "        mid_time = (start_time + end_time) / 2\n",
        "        frame_path = os.path.join(self.frames_dir, f\"{video_id}_{mid_time:.1f}.jpg\")\n",
        "        if not os.path.exists(frame_path):\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "            cap.set(cv2.CAP_PROP_POS_MSEC, mid_time * 1000)\n",
        "            ret, frame = cap.read()\n",
        "            if ret:\n",
        "                cv2.imwrite(frame_path, frame)\n",
        "            cap.release()\n",
        "\n",
        "        return {\n",
        "            \"clip_path\": clip_path,\n",
        "            \"frame_path\": frame_path,\n",
        "            \"start\": start_time,\n",
        "            \"end\": end_time\n",
        "        }\n",
        "\n",
        "    def find_answer(self, question: str, video_paths: List[str]) -> Dict:\n",
        "        \"\"\"Find answer in videos with relevant clips/frames\"\"\"\n",
        "        all_answers = []\n",
        "\n",
        "        for video_path in video_paths:\n",
        "            video_id = os.path.basename(video_path)\n",
        "\n",
        "            # Process video if needed\n",
        "            self.process_video(video_path)\n",
        "            segments = self.video_data[video_id][\"segments\"]\n",
        "\n",
        "            # Get question embedding\n",
        "            q_embedding = self.embedder.encode(question, convert_to_tensor=True)\n",
        "\n",
        "            # Find relevant segments\n",
        "            relevant_segments = []\n",
        "            for seg in segments:\n",
        "                text_embedding = self.embedder.encode(seg[\"text\"], convert_to_tensor=True)\n",
        "                similarity = cosine_similarity(\n",
        "                    q_embedding.cpu().numpy().reshape(1, -1),\n",
        "                    text_embedding.cpu().numpy().reshape(1, -1)\n",
        "                )[0][0]\n",
        "\n",
        "                if similarity > 0.3:  # Similarity threshold\n",
        "                    seg[\"similarity\"] = similarity\n",
        "                    relevant_segments.append(seg)\n",
        "\n",
        "            if not relevant_segments:\n",
        "                continue\n",
        "\n",
        "            # Sort by similarity\n",
        "            relevant_segments.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
        "\n",
        "            # Get best segments\n",
        "            context = \" \".join(seg[\"text\"] for seg in relevant_segments[:2])\n",
        "\n",
        "            # Get answer\n",
        "            answer = self.qa_model(question=question, context=context)\n",
        "\n",
        "            if answer[\"score\"] > 0.1:  # Confidence threshold\n",
        "                # Extract video clip for best matching segment\n",
        "                best_segment = relevant_segments[0]\n",
        "                media = self.extract_clip(\n",
        "                    video_path,\n",
        "                    best_segment[\"start\"],\n",
        "                    best_segment[\"end\"]\n",
        "                )\n",
        "\n",
        "                all_answers.append({\n",
        "                    \"video_id\": video_id,\n",
        "                    \"answer\": answer[\"answer\"],\n",
        "                    \"confidence\": answer[\"score\"],\n",
        "                    \"clip_path\": media[\"clip_path\"],\n",
        "                    \"frame_path\": media[\"frame_path\"],\n",
        "                    \"start_time\": media[\"start\"],\n",
        "                    \"end_time\": media[\"end\"],\n",
        "                    \"context\": best_segment[\"text\"]\n",
        "                })\n",
        "\n",
        "        if not all_answers:\n",
        "            return {\"error\": \"No relevant answer found in videos\"}\n",
        "\n",
        "        # Sort by confidence\n",
        "        all_answers.sort(key=lambda x: x[\"confidence\"], reverse=True)\n",
        "\n",
        "        return {\n",
        "            \"answers\": all_answers,\n",
        "            \"best_answer\": {\n",
        "                \"text\": \" | \".join(f\"From {a['video_id']}: {a['answer']}\" for a in all_answers),\n",
        "                \"confidence\": max(a[\"confidence\"] for a in all_answers)\n",
        "            }\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    # Example usage\n",
        "    qa_system = VideoQA()\n",
        "    videos = [\"/content/test.mp4\", \"/content/videoplayback.mp4\"]\n",
        "\n",
        "    questions = [\n",
        "        \"What is the main topic discussed in the video?\",\n",
        "        \"What are the key points mentioned?\",\n",
        "        \"who is she?\",\n",
        "        \"what she is making?\",\n",
        "        \"why she is cooking?\",\n",
        "        \"how swarty house looks like?\"\n",
        "    ]\n",
        "    # Ensure consistent indentation for the loop\n",
        "    for question in questions:\n",
        "        result = qa_system.find_answer(question, videos)\n",
        "\n",
        "        print(f\"Question: {question}\")\n",
        "        #print(f\"\\nBest Answer: {result['best_answer']['text']}\")\n",
        "        print(f\"Confidence: {result['answer']['confidence']:.2f}\")\n",
        "\n",
        "        print(\"\\nDetailed Answers:\")\n",
        "        for ans in result[\"answers\"]:\n",
        "            print(f\"\\nFrom Video: {ans['video_id']}\")\n",
        "            print(f\"Answer: {ans['answer']}\")\n",
        "            print(f\"Timestamp: {ans['start_time']:.1f}s to {ans['end_time']:.1f}s\")\n",
        "            print(f\"Clip saved at: {ans['clip_path']}\")\n",
        "            print(f\"Frame saved at: {ans['frame_path']}\")\n",
        "            print(f\"Context: {ans['context']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "H2NFUkWkV5Uu",
        "outputId": "5d90fc63-2971-4ba8-e17f-d7ea412720b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "\n",
            "Device set to use cpu\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the main topic discussed in the video?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'answer'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-183d581c224d>\u001b[0m in \u001b[0;36m<cell line: 183>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-183d581c224d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Question: {question}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m#print(f\"\\nBest Answer: {result['best_answer']['text']}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Confidence: {result['answer']['confidence']:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDetailed Answers:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'answer'"
          ]
        }
      ]
    }
  ]
}