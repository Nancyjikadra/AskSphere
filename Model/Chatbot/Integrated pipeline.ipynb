{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4CTcnPA64ky",
        "outputId": "23564861-78d1-45cf-d8b4-d4d7905aeef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-w8vsghsz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-w8vsghsz\n",
            "  Resolved https://github.com/openai/whisper.git to commit dd4d010d2c585bc70aeddd166cd3e26b0bb62f31\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
            "Collecting tiktoken (from openai-whisper==20240930)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.67.1)\n",
            "Collecting triton>=2 (from openai-whisper==20240930)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2->openai-whisper==20240930) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803667 sha256=3802f63ac386aa2ce70821ab5d07df796c3246f4946fe784af08e11e3f46402e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wjh3x9nd/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import librosa\n",
        "import speech_recognition as sr\n",
        "import moviepy.editor as mp\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import re\n",
        "\n",
        "class AdvancedTokenCleaner:\n",
        "    @staticmethod\n",
        "    def clean_tokens(tokens):\n",
        "        \"\"\"\n",
        "        Intelligently clean and filter tokens\n",
        "\n",
        "        Args:\n",
        "            tokens (list): Raw tokens from tokenizer\n",
        "\n",
        "        Returns:\n",
        "            list: Cleaned meaningful tokens\n",
        "        \"\"\"\n",
        "        # Remove special tokens and whitespace tokens\n",
        "        cleaned_tokens = []\n",
        "\n",
        "        # Define filtering criteria\n",
        "        def is_meaningful_token(token):\n",
        "            # Remove special characters, very short tokens, and whitespace\n",
        "            if token.startswith('##'):\n",
        "                token = token.replace('##', '')\n",
        "\n",
        "            # Criteria for meaningful tokens\n",
        "            return (\n",
        "                len(token) > 1 and  # Minimum length\n",
        "                not token.isspace() and  # Not just whitespace\n",
        "                not token.startswith('[') and  # Not special tokens\n",
        "                not token.endswith(']') and\n",
        "                not token in ['<pad>', '<s>', '</s>']  # Remove padding tokens\n",
        "            )\n",
        "\n",
        "        # Filter and process tokens\n",
        "        for token in tokens:\n",
        "            if is_meaningful_token(token):\n",
        "                # Remove any remaining special characters\n",
        "                cleaned_token = re.sub(r'[^a-zA-Z0-9\\u0900-\\u097F]', '', token)\n",
        "\n",
        "                if cleaned_token:\n",
        "                    cleaned_tokens.append(cleaned_token)\n",
        "\n",
        "        return cleaned_tokens\n",
        "\n",
        "class MultilingualTokenizer:\n",
        "    def __init__(self, model_name=\"google/mt5-base\"):\n",
        "        \"\"\"\n",
        "        Initialize multilingual tokenizer with advanced cleaning\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            self.model = AutoModel.from_pretrained(model_name)\n",
        "            self.token_cleaner = AdvancedTokenCleaner()\n",
        "        except Exception as e:\n",
        "            print(f\"Tokenizer loading error: {e}\")\n",
        "            self.tokenizer = None\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        \"\"\"\n",
        "        Advanced text tokenization with intelligent cleaning\n",
        "\n",
        "        Args:\n",
        "            text (str): Input text to tokenize\n",
        "\n",
        "        Returns:\n",
        "            dict: Tokenization results\n",
        "        \"\"\"\n",
        "        if not self.tokenizer or not text:\n",
        "            return None\n",
        "\n",
        "        # Preprocess text (remove extra whitespaces)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Tokenize text\n",
        "        tokens = self.tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            add_special_tokens=True\n",
        "        )\n",
        "\n",
        "        # Convert to tokens and clean\n",
        "        raw_tokens = self.tokenizer.convert_ids_to_tokens(tokens['input_ids'][0])\n",
        "\n",
        "        # Clean tokens\n",
        "        cleaned_tokens = self.token_cleaner.clean_tokens(raw_tokens)\n",
        "\n",
        "        return {\n",
        "            'input_ids': tokens['input_ids'],\n",
        "            'attention_mask': tokens['attention_mask'],\n",
        "            'raw_tokens': raw_tokens,\n",
        "            'cleaned_tokens': cleaned_tokens\n",
        "        }\n",
        "\n",
        "class SpeechToTextExtractor:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize speech recognition with advanced processing\n",
        "        \"\"\"\n",
        "        self.recognizer = sr.Recognizer()\n",
        "\n",
        "        # Configure recognizer for better accuracy\n",
        "        self.recognizer.dynamic_energy_threshold = True\n",
        "        self.recognizer.pause_threshold = 0.5\n",
        "\n",
        "    def extract_speech_from_audio(self, audio_path, language='en-IN'):\n",
        "        \"\"\"\n",
        "        Extract speech text with multiple recognition attempts\n",
        "\n",
        "        Args:\n",
        "            audio_path (str): Path to audio file\n",
        "            language (str): Language for speech recognition\n",
        "\n",
        "        Returns:\n",
        "            str: Extracted speech text\n",
        "        \"\"\"\n",
        "        try:\n",
        "            with sr.AudioFile(audio_path) as source:\n",
        "                # Adjust for ambient noise\n",
        "                self.recognizer.adjust_for_ambient_noise(source, duration=1)\n",
        "\n",
        "                # Read the entire audio file\n",
        "                audio = self.recognizer.record(source)\n",
        "\n",
        "                # Multiple recognition attempts\n",
        "                recognition_methods = [\n",
        "                    lambda: self.recognizer.recognize_google(audio, language=language),\n",
        "                    lambda: self.recognizer.recognize_sphinx(audio)\n",
        "                ]\n",
        "\n",
        "                for method in recognition_methods:\n",
        "                    try:\n",
        "                        text = method()\n",
        "                        if text:\n",
        "                            return text\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Speech recognition error: {e}\")\n",
        "            return None\n",
        "\n",
        "class AudioPreprocessor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        sample_rate=16000,\n",
        "        max_duration=60\n",
        "    ):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.max_duration = max_duration\n",
        "        self.speech_extractor = SpeechToTextExtractor()\n",
        "\n",
        "    def extract_audio(self, video_path):\n",
        "        \"\"\"\n",
        "        Extract audio from video with comprehensive processing\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to input video\n",
        "\n",
        "        Returns:\n",
        "            tuple: Audio data and extracted text\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Extract audio from video\n",
        "            video = mp.VideoFileClip(video_path)\n",
        "            temp_audio_path = \"temp_audio.wav\"\n",
        "            video.audio.write_audiofile(temp_audio_path)\n",
        "\n",
        "            # Extract speech text\n",
        "            extracted_text = self.speech_extractor.extract_speech_from_audio(temp_audio_path)\n",
        "\n",
        "            # Load audio data\n",
        "            audio_data, _ = librosa.load(\n",
        "                temp_audio_path,\n",
        "                sr=self.sample_rate,\n",
        "                duration=self.max_duration\n",
        "            )\n",
        "\n",
        "            # Clean up temporary file\n",
        "            os.remove(temp_audio_path)\n",
        "\n",
        "            return audio_data, extracted_text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Audio extraction error: {e}\")\n",
        "            return None, None\n",
        "\n",
        "class AudioTokenizationPipeline:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize comprehensive tokenization pipeline\n",
        "        \"\"\"\n",
        "        self.preprocessor = AudioPreprocessor()\n",
        "        self.text_tokenizer = MultilingualTokenizer()\n",
        "\n",
        "    def tokenize_video(self, video_path):\n",
        "        \"\"\"\n",
        "        Comprehensive video tokenization\n",
        "\n",
        "        Args:\n",
        "            video_path (str): Path to input video\n",
        "\n",
        "        Returns:\n",
        "            dict: Detailed tokenization results\n",
        "        \"\"\"\n",
        "        # Extract audio and speech text\n",
        "        audio_data, extracted_text = self.preprocessor.extract_audio(video_path)\n",
        "\n",
        "        if audio_data is None or extracted_text is None:\n",
        "            print(\"Failed to extract audio or speech\")\n",
        "            return None\n",
        "\n",
        "        # Tokenize extracted text\n",
        "        text_tokens = self.text_tokenizer.tokenize_text(extracted_text)\n",
        "\n",
        "        # Print comprehensive results\n",
        "        print(\"\\n--- Video Speech Extraction ---\")\n",
        "        video_transcripts.insert(extrected_text)\n",
        "        print(\"Extracted Text:\", extracted_text)\n",
        "\n",
        "        if text_tokens:\n",
        "            print(\"\\n--- Token Details ---\")\n",
        "            print(\"Raw Tokens:\", text_tokens['raw_tokens'])\n",
        "            print(\"Cleaned Tokens:\", text_tokens['cleaned_tokens'])\n",
        "            print(\"Token IDs Shape:\", text_tokens['input_ids'].shape)\n",
        "\n",
        "        return {\n",
        "            'audio_data': audio_data,\n",
        "            'extracted_text': extracted_text,\n",
        "            'text_tokens': text_tokens\n",
        "        }\n",
        "\n",
        "#video selection\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def video_selection(question):\n",
        "  # Load pre-trained model and tokenizer for embeddings\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "  model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "  def get_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of texts.\n",
        "    \"\"\"\n",
        "      inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "      outputs = model(**inputs)\n",
        "      return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
        "\n",
        "\n",
        "# Step 2: Embed video transcripts\n",
        "  video_embeddings = get_embeddings(video_transcripts)\n",
        "\n",
        "# Step 3: Embed user question\n",
        "  question_embedding = get_embeddings([question])\n",
        "\n",
        "# Step 4: Compute relevance scores\n",
        "  similarities = cosine_similarity(question_embedding, video_embeddings)\n",
        "\n",
        "# Step 5: Select top-K videos\n",
        "  top_k = 2  # Number of videos to select\n",
        "  top_k_indices = np.argsort(similarities[0])[::-1][:top_k]\n",
        "  selected_videos = [video_transcripts[i] for i in top_k_indices]\n",
        "\n",
        "# Output the selected videos\n",
        "  print(\"Selected Videos:\")\n",
        "  for i, video in enumerate(selected_videos, 1):\n",
        "      print(f\"{i}. {video}\")\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import warnings\n",
        "from typing import List, Dict, Union, Optional\n",
        "from transformers import pipeline\n",
        "import whisper\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from moviepy.editor import VideoFileClip\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "\n",
        "class FixedVideoChatbot:\n",
        "    def __init__(self, cache_dir: str = \"video_cache\", log_file: str = \"chatbot.log\"):\n",
        "        self.setup_logging(log_file)\n",
        "        self.cache_dir = cache_dir\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "        try:\n",
        "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "            self.logger.info(f\"Using device: {self.device}\")\n",
        "\n",
        "            self.transcription_model = whisper.load_model(\"base\", device=self.device)\n",
        "            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            self.qa_pipeline = pipeline(\n",
        "                \"question-answering\",\n",
        "                model=\"deepset/roberta-base-squad2\",\n",
        "                device=0 if torch.cuda.is_available() else -1\n",
        "            )\n",
        "            self.logger.info(\"Models loaded successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error loading models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        self.cached_data = {}\n",
        "\n",
        "    def setup_logging(self, log_file: str):\n",
        "        self.logger = logging.getLogger('VideoChatbot')\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_handler.setFormatter(formatter)\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "    def extract_audio(self, video_path: str) -> np.ndarray:\n",
        "        \"\"\"Extract audio using librosa instead of moviepy.\"\"\"\n",
        "        try:\n",
        "            # Extract audio using librosa\n",
        "            self.logger.info(f\"Extracting audio from {video_path}\")\n",
        "            audio_array, sr = librosa.load(video_path, sr=16000, mono=True)\n",
        "\n",
        "            # Ensure audio is the correct format for whisper\n",
        "            if not isinstance(audio_array, np.ndarray):\n",
        "                raise ValueError(\"Audio extraction failed\")\n",
        "\n",
        "            if len(audio_array.shape) != 1:\n",
        "                audio_array = audio_array.mean(axis=-1)\n",
        "\n",
        "            return audio_array.astype(np.float32)\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error in audio extraction: {str(e)}\")\n",
        "            try:\n",
        "                # Fallback method using moviepy\n",
        "                self.logger.info(\"Attempting fallback audio extraction\")\n",
        "                with VideoFileClip(video_path) as video:\n",
        "                    if video.audio is None:\n",
        "                        raise ValueError(\"Video has no audio track\")\n",
        "                    audio_array = video.audio.to_soundarray(fps=16000)\n",
        "                    if len(audio_array.shape) > 1:\n",
        "                        audio_array = audio_array.mean(axis=1)\n",
        "                    return audio_array.astype(np.float32)\n",
        "            except Exception as fallback_error:\n",
        "                self.logger.error(f\"Fallback audio extraction failed: {str(fallback_error)}\")\n",
        "                raise\n",
        "\n",
        "    def process_chunks(self, audio_array: np.ndarray, chunk_duration: int = 30) -> List[Dict]:\n",
        "        \"\"\"Process audio in chunks to handle memory constraints.\"\"\"\n",
        "        chunk_size = chunk_duration * 16000  # assuming 16kHz sample rate\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(audio_array), chunk_size):\n",
        "            chunk = audio_array[i:i + chunk_size]\n",
        "            if len(chunk) < 100:  # Skip very small chunks\n",
        "                continue\n",
        "\n",
        "            # Ensure chunk is 1D array\n",
        "            if len(chunk.shape) > 1:\n",
        "                chunk = chunk.mean(axis=-1)\n",
        "\n",
        "            result = self.transcription_model.transcribe(chunk)\n",
        "\n",
        "            # Adjust timestamps\n",
        "            for segment in result[\"segments\"]:\n",
        "                segment[\"start\"] += i / 16000  # Convert samples to seconds\n",
        "                segment[\"end\"] += i / 16000\n",
        "                chunks.append(segment)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def process_video(self, video_path: str) -> Dict:\n",
        "        \"\"\"Process video with improved error handling.\"\"\"\n",
        "        try:\n",
        "            video_id = os.path.basename(video_path)\n",
        "            cache_file = os.path.join(self.cache_dir, f\"{video_id}_transcription.json\")\n",
        "\n",
        "            # Check cache\n",
        "            if os.path.exists(cache_file):\n",
        "                with open(cache_file, 'r') as f:\n",
        "                    return json.load(f)\n",
        "\n",
        "            # Extract and process audio\n",
        "            audio_array = self.extract_audio(video_path)\n",
        "\n",
        "            # Process in chunks\n",
        "            transcription_segments = self.process_chunks(audio_array)\n",
        "\n",
        "            # Combine results\n",
        "            transcription_data = {\n",
        "                \"full_text\": \" \".join(seg[\"text\"] for seg in transcription_segments),\n",
        "                \"segments\": transcription_segments\n",
        "            }\n",
        "\n",
        "            # Cache results\n",
        "            with open(cache_file, 'w') as f:\n",
        "                json.dump(transcription_data, f)\n",
        "\n",
        "            return transcription_data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error processing video {video_path}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def find_relevant_context(self, query: str, transcription_data: Dict) -> str:\n",
        "        \"\"\"Find relevant context with improved error handling.\"\"\"\n",
        "        try:\n",
        "            if not transcription_data[\"segments\"]:\n",
        "                return \"\"\n",
        "\n",
        "            # Convert query to embedding\n",
        "            query_embedding = self.embedding_model.encode([query])\n",
        "\n",
        "            # Get segment texts and embeddings\n",
        "            segment_texts = [s[\"text\"] for s in transcription_data[\"segments\"]]\n",
        "            if not segment_texts:\n",
        "                return \"\"\n",
        "\n",
        "            segment_embeddings = self.embedding_model.encode(segment_texts)\n",
        "\n",
        "            # Ensure proper shapes\n",
        "            if len(query_embedding.shape) == 1:\n",
        "                query_embedding = query_embedding.reshape(1, -1)\n",
        "            if len(segment_embeddings.shape) == 1:\n",
        "                segment_embeddings = segment_embeddings.reshape(1, -1)\n",
        "\n",
        "            # Calculate similarities\n",
        "            similarities = cosine_similarity(query_embedding, segment_embeddings)[0]\n",
        "\n",
        "            # Get top segments\n",
        "            top_k = min(3, len(similarities))\n",
        "            top_indices = np.argsort(similarities)[-top_k:]\n",
        "\n",
        "            # Combine context\n",
        "            context = \" \".join(segment_texts[i] for i in top_indices)\n",
        "            return context\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error finding context: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "    def get_response(self, query: str, video_paths: Union[str, List[str]]) -> Dict:\n",
        "        \"\"\"Generate response with improved error handling.\"\"\"\n",
        "        try:\n",
        "            if isinstance(video_paths, str):\n",
        "                video_paths = [video_paths]\n",
        "\n",
        "            all_contexts = []\n",
        "            for video_path in video_paths:\n",
        "                if video_path not in self.cached_data:\n",
        "                    self.cached_data[video_path] = self.process_video(video_path)\n",
        "\n",
        "                context = self.find_relevant_context(query, self.cached_data[video_path])\n",
        "                if context:\n",
        "                    all_contexts.append(context)\n",
        "\n",
        "            if not all_contexts:\n",
        "                return {\n",
        "                    \"answer\": \"No relevant information found in the videos.\",\n",
        "                    \"confidence\": 0.0,\n",
        "                    \"context\": \"\"\n",
        "                }\n",
        "\n",
        "            combined_context = \" \".join(all_contexts)\n",
        "\n",
        "            # Generate answer\n",
        "            answer = self.qa_pipeline(\n",
        "                question=query,\n",
        "                context=combined_context,\n",
        "                max_answer_length=100\n",
        "            )\n",
        "\n",
        "            return {\n",
        "                \"answer\": answer[\"answer\"],\n",
        "                \"confidence\": answer[\"score\"],\n",
        "                \"context\": combined_context\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error generating response: {str(e)}\")\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"answer\": \"Sorry, I encountered an error processing your question.\",\n",
        "                \"confidence\": 0.0\n",
        "            }\n",
        "\n",
        "def main():\n",
        "    # Video path\n",
        "    video_paths = [\"/content/How to control sound waves.mp4\",\"/content/videoplayback (2).mp4\",\"/content/videoplayback.mp4\"]\n",
        "\n",
        "    for video_path in video_paths:\n",
        "      # Initialize pipeline\n",
        "      pipeline = AudioTokenizationPipeline()\n",
        "\n",
        "      try:\n",
        "          # Tokenize video\n",
        "          tokenization_result = pipeline.tokenize_video(video_path)\n",
        "\n",
        "          if tokenization_result:\n",
        "              # Additional processing or analysis can be done here\n",
        "              pass\n",
        "\n",
        "      except Exception as e:\n",
        "          print(f\"Tokenization failed: {e}\")\n",
        "\n",
        "\n",
        "    for question in questions:\n",
        "      pipeline2 = video_selection()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7u3unYOK3jW3",
        "outputId": "3400331b-f875-49d7-a15e-52069301b070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Videos:\n",
            "1. 2 hour lecture so I'm headed to my sorority house to get some lunch as we know I have been sick so I need to make sure I'm eating with my medicine I think today is meatball subs but I'm just going to get meatballs cuz those are my favorite parts let's go together we made it the decorations are gorgeous thank you guys later bye guys\n",
            "2. Learning machine learning is need of todays time, it is no more skill now it is necessity.\n",
            "\n",
            "Chatbot Response:\n",
            "I found 2 relevant videos for your question:\n",
            "1. 2 hour lecture so I'm headed to my sorority house to get some lunch as we know I have been sick so I need to make sure I'm eating with my medicine I think today is meatball subs but I'm just going to get meatballs cuz those are my favorite parts let's go together we made it the decorations are gorgeous thank you guys later bye guys\n",
            "2. Learning machine learning is need of todays time, it is no more skill now it is necessity.\n",
            "\n",
            "Here’s a brief answer based on these videos:\n",
            "2 hour lecture so I'm headed to my sorority house to get some lunch as we know I have been sick so I need to make sure I'm eating with my medicine I think today is meatball subs but I'm just going to get meatballs cuz those are my favorite parts let's go together we made it the decorations are gorge...\n"
          ]
        }
      ]
    }
  ]
}